{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gym\n",
    "import gc\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "\n",
    "gc.enable()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype('float').ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL INITIALIZATION\n",
    "\n",
    "# Hyperparams\n",
    "H = 200 # n hidden layer neurons\n",
    "batch_size = 16 # n episodes before param update\n",
    "learning_rate = 1e-3\n",
    "gamma = 0.99 # discount factor\n",
    "b1 = 0.9\n",
    "b2 = 0.999\n",
    "epsilon = 1e-7\n",
    "resume = False # resume from previous checkpoint?\n",
    "path = ''\n",
    "render = False\n",
    "\n",
    "# Model initialization\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "if resume:\n",
    "    model = pickle.load(open(f'checkpoints/{path}.p','rb'))\n",
    "else:\n",
    "    model = {}\n",
    "    model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization, centers around 0\n",
    "    model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "\n",
    "# Tensorboard logging\n",
    "# Writer outputs to ./runs/\n",
    "writer = SummaryWriter('exp3')\n",
    "def log_reward(episode,r):\n",
    "  writer.add_scalar('reward:',r,episode)\n",
    "\n",
    "# Tracks gradients, 1st moment, and 2nd moment over a batch\n",
    "gradient_buffer = { k : np.zeros_like(v) for k,v in model.items() }\n",
    "m = { k : np.zeros_like(v) for k,v in model.items() }\n",
    "c = { k : np.zeros_like(v) for k,v in model.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PG FUNCTIONS\n",
    "\n",
    "def discount_rewards(r):\n",
    "  \"\"\" take 1D float array of rewards over episode and compute discounted reward \"\"\"\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  G = 0\n",
    "\n",
    "  # Working backwards from the terminal state\n",
    "  for t in reversed(range(0, r.size)):\n",
    "    # (Loosely) update value at each state\n",
    "    G = gamma * G + r[t]\n",
    "    discounted_r[t] = G\n",
    "\n",
    "  return discounted_r\n",
    "\n",
    "def policy_forward(x):\n",
    "  \"\"\" given frame, return probability of action 2 \"\"\"\n",
    "  # Layer 1\n",
    "  h = np.dot(model['W1'], x)\n",
    "  # ReLU\n",
    "  h[h<0] = 0\n",
    "  # Layer 2: get logits\n",
    "  logp = np.dot(model['W2'], h)\n",
    "  # Sigmoid\n",
    "  p = sigmoid(logp)\n",
    "\n",
    "  return p, h # Return probability of action 2, hidden state\n",
    "\n",
    "def policy_backward(h, x, pgrad):\n",
    "  \"\"\"\n",
    "  backward pass that gets policy gradients\n",
    "  \n",
    "  h: hidden states\n",
    "  x: observed states\n",
    "  pgrad: policy gradients\n",
    "\n",
    "  \"\"\"\n",
    "  # Second layer gradients\n",
    "  dW2 = np.dot(h.T, pgrad).ravel()\n",
    "  dh = np.outer(pgrad, model['W2'])\n",
    "  dh[h <= 0] = 0\n",
    "\n",
    "  # First layer gradients\n",
    "  dW1 = np.dot(dh.T, x)\n",
    "  return {'W1':dW1, 'W2':dW2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE GYM\n",
    "\n",
    "env = gym.make(\"Pong-v4\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "prev_x = None # Used for differencing\n",
    "xs,hs,pgrads,rs = [],[],[],[] # Observed, hidden, policy gradient, rewards\n",
    "xsp,hsp,pgradsp,rsp = [],[],[],[] # For positive episodes\n",
    "running_reward = -21\n",
    "reward_sum_neg = 0\n",
    "reward_sum_pos = 0\n",
    "episode_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define gradient update\n",
    "def gradient_update(rs: List,\n",
    "                    xs: List,\n",
    "                    hs: List,\n",
    "                    pgrads: List,\n",
    "                    gradient_buffer: Dict):\n",
    "\n",
    "    # Stack negative intermediaries\n",
    "    ixs = np.vstack(xs)\n",
    "    ihs = np.vstack(hs)\n",
    "    ipgrads = np.vstack(pgrads)\n",
    "    irs = np.vstack(rs)\n",
    "\n",
    "    # Get discounted rewards\n",
    "    discounted_r = discount_rewards(irs)\n",
    "    # Normalize rewards\n",
    "    discounted_r -= np.mean(discounted_r)\n",
    "    discounted_r /= np.std(discounted_r)\n",
    "\n",
    "    # Calculate gradients (using Advantage and Policy Gradients)\n",
    "    ipgrads *= discounted_r\n",
    "    grad = policy_backward(ihs, ixs, ipgrads)\n",
    "\n",
    "    # Accumulate gradients over batch\n",
    "    for k in model:\n",
    "      gradient_buffer[k] += grad[k]\n",
    "    \n",
    "    return gradient_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "gradient_buffer = { k : np.zeros_like(v) for k,v in iter(model.items()) } # update buffers that add up gradients over a batch\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in iter(model.items()) } # rmsprop memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode reward total was -19.00. running mean: -20.98\n",
      "resetting env. episode reward total was -21.00. running mean: -20.98\n",
      "resetting env. episode reward total was -21.00. running mean: -20.98\n",
      "resetting env. episode reward total was -17.00. running mean: -20.94\n",
      "resetting env. episode reward total was -18.00. running mean: -20.91\n",
      "resetting env. episode reward total was -20.00. running mean: -20.90\n",
      "resetting env. episode reward total was -20.00. running mean: -20.89\n",
      "resetting env. episode reward total was -20.00. running mean: -20.88\n",
      "resetting env. episode reward total was -21.00. running mean: -20.89\n",
      "resetting env. episode reward total was -20.00. running mean: -20.88\n",
      "resetting env. episode reward total was -21.00. running mean: -20.88\n",
      "resetting env. episode reward total was -20.00. running mean: -20.87\n",
      "resetting env. episode reward total was -21.00. running mean: -20.87\n",
      "resetting env. episode reward total was -21.00. running mean: -20.87\n",
      "resetting env. episode reward total was -20.00. running mean: -20.86\n",
      "resetting env. episode reward total was -21.00. running mean: -20.86\n",
      "resetting env. episode reward total was -21.00. running mean: -20.87\n",
      "resetting env. episode reward total was -21.00. running mean: -20.87\n",
      "resetting env. episode reward total was -20.00. running mean: -20.86\n",
      "resetting env. episode reward total was -18.00. running mean: -20.83\n",
      "resetting env. episode reward total was -20.00. running mean: -20.82\n",
      "resetting env. episode reward total was -21.00. running mean: -20.82\n",
      "resetting env. episode reward total was -20.00. running mean: -20.81\n",
      "resetting env. episode reward total was -19.00. running mean: -20.80\n",
      "resetting env. episode reward total was -18.00. running mean: -20.77\n",
      "resetting env. episode reward total was -20.00. running mean: -20.76\n",
      "resetting env. episode reward total was -20.00. running mean: -20.75\n",
      "resetting env. episode reward total was -21.00. running mean: -20.76\n",
      "resetting env. episode reward total was -21.00. running mean: -20.76\n",
      "resetting env. episode reward total was -20.00. running mean: -20.75\n",
      "resetting env. episode reward total was -21.00. running mean: -20.75\n",
      "resetting env. episode reward total was -19.00. running mean: -20.74\n",
      "resetting env. episode reward total was -19.00. running mean: -20.72\n",
      "resetting env. episode reward total was -21.00. running mean: -20.72\n",
      "resetting env. episode reward total was -19.00. running mean: -20.70\n",
      "resetting env. episode reward total was -21.00. running mean: -20.71\n",
      "resetting env. episode reward total was -20.00. running mean: -20.70\n",
      "resetting env. episode reward total was -21.00. running mean: -20.70\n",
      "resetting env. episode reward total was -21.00. running mean: -20.71\n",
      "resetting env. episode reward total was -21.00. running mean: -20.71\n",
      "resetting env. episode reward total was -21.00. running mean: -20.71\n",
      "resetting env. episode reward total was -20.00. running mean: -20.70\n",
      "resetting env. episode reward total was -21.00. running mean: -20.71\n",
      "resetting env. episode reward total was -21.00. running mean: -20.71\n",
      "resetting env. episode reward total was -19.00. running mean: -20.69\n",
      "resetting env. episode reward total was -21.00. running mean: -20.70\n",
      "resetting env. episode reward total was -20.00. running mean: -20.69\n",
      "resetting env. episode reward total was -21.00. running mean: -20.69\n",
      "resetting env. episode reward total was -20.00. running mean: -20.69\n",
      "resetting env. episode reward total was -20.00. running mean: -20.68\n",
      "resetting env. episode reward total was -21.00. running mean: -20.68\n",
      "resetting env. episode reward total was -20.00. running mean: -20.68\n",
      "resetting env. episode reward total was -21.00. running mean: -20.68\n",
      "resetting env. episode reward total was -19.00. running mean: -20.66\n",
      "resetting env. episode reward total was -21.00. running mean: -20.67\n",
      "resetting env. episode reward total was -21.00. running mean: -20.67\n",
      "resetting env. episode reward total was -21.00. running mean: -20.67\n",
      "resetting env. episode reward total was -18.00. running mean: -20.64\n",
      "resetting env. episode reward total was -21.00. running mean: -20.65\n",
      "resetting env. episode reward total was -21.00. running mean: -20.65\n",
      "resetting env. episode reward total was -21.00. running mean: -20.66\n",
      "resetting env. episode reward total was -21.00. running mean: -20.66\n",
      "resetting env. episode reward total was -21.00. running mean: -20.66\n",
      "resetting env. episode reward total was -21.00. running mean: -20.67\n",
      "resetting env. episode reward total was -21.00. running mean: -20.67\n",
      "resetting env. episode reward total was -21.00. running mean: -20.67\n",
      "resetting env. episode reward total was -21.00. running mean: -20.68\n",
      "resetting env. episode reward total was -20.00. running mean: -20.67\n",
      "resetting env. episode reward total was -21.00. running mean: -20.67\n",
      "resetting env. episode reward total was -21.00. running mean: -20.68\n",
      "resetting env. episode reward total was -20.00. running mean: -20.67\n",
      "resetting env. episode reward total was -20.00. running mean: -20.66\n",
      "resetting env. episode reward total was -21.00. running mean: -20.67\n",
      "resetting env. episode reward total was -21.00. running mean: -20.67\n",
      "resetting env. episode reward total was -21.00. running mean: -20.67\n",
      "resetting env. episode reward total was -20.00. running mean: -20.67\n",
      "resetting env. episode reward total was -20.00. running mean: -20.66\n",
      "resetting env. episode reward total was -21.00. running mean: -20.66\n",
      "resetting env. episode reward total was -20.00. running mean: -20.66\n",
      "resetting env. episode reward total was -21.00. running mean: -20.66\n",
      "resetting env. episode reward total was -21.00. running mean: -20.66\n",
      "resetting env. episode reward total was -19.00. running mean: -20.65\n",
      "resetting env. episode reward total was -21.00. running mean: -20.65\n",
      "resetting env. episode reward total was -20.00. running mean: -20.64\n",
      "resetting env. episode reward total was -21.00. running mean: -20.65\n",
      "resetting env. episode reward total was -21.00. running mean: -20.65\n",
      "resetting env. episode reward total was -20.00. running mean: -20.64\n",
      "resetting env. episode reward total was -21.00. running mean: -20.65\n",
      "resetting env. episode reward total was -21.00. running mean: -20.65\n",
      "resetting env. episode reward total was -21.00. running mean: -20.65\n",
      "resetting env. episode reward total was -20.00. running mean: -20.65\n",
      "resetting env. episode reward total was -20.00. running mean: -20.64\n",
      "resetting env. episode reward total was -21.00. running mean: -20.64\n",
      "resetting env. episode reward total was -18.00. running mean: -20.62\n",
      "resetting env. episode reward total was -20.00. running mean: -20.61\n",
      "resetting env. episode reward total was -21.00. running mean: -20.62\n",
      "resetting env. episode reward total was -21.00. running mean: -20.62\n",
      "resetting env. episode reward total was -19.00. running mean: -20.60\n",
      "resetting env. episode reward total was -20.00. running mean: -20.60\n",
      "resetting env. episode reward total was -21.00. running mean: -20.60\n",
      "resetting env. episode reward total was -21.00. running mean: -20.61\n",
      "resetting env. episode reward total was -19.00. running mean: -20.59\n",
      "resetting env. episode reward total was -20.00. running mean: -20.58\n",
      "resetting env. episode reward total was -20.00. running mean: -20.58\n",
      "resetting env. episode reward total was -21.00. running mean: -20.58\n",
      "resetting env. episode reward total was -20.00. running mean: -20.58\n",
      "resetting env. episode reward total was -20.00. running mean: -20.57\n",
      "resetting env. episode reward total was -21.00. running mean: -20.57\n",
      "resetting env. episode reward total was -20.00. running mean: -20.57\n",
      "resetting env. episode reward total was -21.00. running mean: -20.57\n",
      "resetting env. episode reward total was -21.00. running mean: -20.58\n",
      "resetting env. episode reward total was -21.00. running mean: -20.58\n",
      "resetting env. episode reward total was -21.00. running mean: -20.59\n",
      "resetting env. episode reward total was -21.00. running mean: -20.59\n",
      "resetting env. episode reward total was -19.00. running mean: -20.57\n",
      "resetting env. episode reward total was -20.00. running mean: -20.57\n",
      "resetting env. episode reward total was -21.00. running mean: -20.57\n",
      "resetting env. episode reward total was -20.00. running mean: -20.57\n",
      "resetting env. episode reward total was -20.00. running mean: -20.56\n",
      "resetting env. episode reward total was -21.00. running mean: -20.57\n",
      "resetting env. episode reward total was -21.00. running mean: -20.57\n",
      "resetting env. episode reward total was -18.00. running mean: -20.54\n",
      "resetting env. episode reward total was -21.00. running mean: -20.55\n",
      "resetting env. episode reward total was -21.00. running mean: -20.55\n",
      "resetting env. episode reward total was -21.00. running mean: -20.56\n",
      "resetting env. episode reward total was -21.00. running mean: -20.56\n",
      "resetting env. episode reward total was -20.00. running mean: -20.56\n",
      "resetting env. episode reward total was -20.00. running mean: -20.55\n",
      "resetting env. episode reward total was -20.00. running mean: -20.55\n",
      "resetting env. episode reward total was -21.00. running mean: -20.55\n",
      "resetting env. episode reward total was -21.00. running mean: -20.55\n",
      "resetting env. episode reward total was -21.00. running mean: -20.56\n",
      "resetting env. episode reward total was -20.00. running mean: -20.55\n",
      "resetting env. episode reward total was -21.00. running mean: -20.56\n",
      "resetting env. episode reward total was -19.00. running mean: -20.54\n",
      "resetting env. episode reward total was -20.00. running mean: -20.54\n",
      "resetting env. episode reward total was -21.00. running mean: -20.54\n",
      "resetting env. episode reward total was -21.00. running mean: -20.55\n",
      "resetting env. episode reward total was -21.00. running mean: -20.55\n",
      "resetting env. episode reward total was -21.00. running mean: -20.56\n",
      "resetting env. episode reward total was -20.00. running mean: -20.55\n",
      "resetting env. episode reward total was -18.00. running mean: -20.52\n",
      "resetting env. episode reward total was -21.00. running mean: -20.53\n",
      "resetting env. episode reward total was -21.00. running mean: -20.53\n",
      "resetting env. episode reward total was -21.00. running mean: -20.54\n",
      "resetting env. episode reward total was -20.00. running mean: -20.53\n",
      "resetting env. episode reward total was -21.00. running mean: -20.54\n",
      "resetting env. episode reward total was -21.00. running mean: -20.54\n",
      "resetting env. episode reward total was -21.00. running mean: -20.55\n",
      "resetting env. episode reward total was -20.00. running mean: -20.54\n",
      "resetting env. episode reward total was -18.00. running mean: -20.52\n",
      "resetting env. episode reward total was -21.00. running mean: -20.52\n",
      "resetting env. episode reward total was -21.00. running mean: -20.53\n",
      "resetting env. episode reward total was -20.00. running mean: -20.52\n",
      "resetting env. episode reward total was -21.00. running mean: -20.53\n",
      "resetting env. episode reward total was -20.00. running mean: -20.52\n",
      "resetting env. episode reward total was -21.00. running mean: -20.52\n",
      "resetting env. episode reward total was -21.00. running mean: -20.53\n",
      "resetting env. episode reward total was -20.00. running mean: -20.52\n",
      "resetting env. episode reward total was -20.00. running mean: -20.52\n",
      "resetting env. episode reward total was -21.00. running mean: -20.52\n",
      "resetting env. episode reward total was -20.00. running mean: -20.52\n",
      "resetting env. episode reward total was -21.00. running mean: -20.52\n",
      "resetting env. episode reward total was -20.00. running mean: -20.52\n",
      "resetting env. episode reward total was -20.00. running mean: -20.51\n",
      "resetting env. episode reward total was -21.00. running mean: -20.52\n",
      "resetting env. episode reward total was -20.00. running mean: -20.51\n",
      "resetting env. episode reward total was -20.00. running mean: -20.51\n",
      "resetting env. episode reward total was -20.00. running mean: -20.50\n",
      "resetting env. episode reward total was -21.00. running mean: -20.51\n",
      "resetting env. episode reward total was -21.00. running mean: -20.51\n",
      "resetting env. episode reward total was -20.00. running mean: -20.51\n",
      "resetting env. episode reward total was -20.00. running mean: -20.50\n",
      "resetting env. episode reward total was -20.00. running mean: -20.50\n",
      "resetting env. episode reward total was -20.00. running mean: -20.49\n",
      "resetting env. episode reward total was -20.00. running mean: -20.49\n",
      "resetting env. episode reward total was -20.00. running mean: -20.48\n",
      "resetting env. episode reward total was -20.00. running mean: -20.48\n",
      "resetting env. episode reward total was -20.00. running mean: -20.47\n",
      "resetting env. episode reward total was -21.00. running mean: -20.48\n",
      "resetting env. episode reward total was -21.00. running mean: -20.48\n",
      "resetting env. episode reward total was -21.00. running mean: -20.49\n",
      "resetting env. episode reward total was -21.00. running mean: -20.49\n",
      "resetting env. episode reward total was -20.00. running mean: -20.49\n",
      "resetting env. episode reward total was -20.00. running mean: -20.48\n",
      "resetting env. episode reward total was -19.00. running mean: -20.47\n",
      "resetting env. episode reward total was -19.00. running mean: -20.45\n",
      "resetting env. episode reward total was -20.00. running mean: -20.45\n",
      "resetting env. episode reward total was -21.00. running mean: -20.46\n",
      "resetting env. episode reward total was -21.00. running mean: -20.46\n",
      "resetting env. episode reward total was -19.00. running mean: -20.45\n",
      "resetting env. episode reward total was -19.00. running mean: -20.43\n",
      "resetting env. episode reward total was -17.00. running mean: -20.40\n",
      "resetting env. episode reward total was -21.00. running mean: -20.40\n",
      "resetting env. episode reward total was -18.00. running mean: -20.38\n",
      "resetting env. episode reward total was -20.00. running mean: -20.38\n",
      "resetting env. episode reward total was -20.00. running mean: -20.37\n",
      "resetting env. episode reward total was -20.00. running mean: -20.37\n",
      "resetting env. episode reward total was -21.00. running mean: -20.37\n",
      "resetting env. episode reward total was -21.00. running mean: -20.38\n",
      "resetting env. episode reward total was -20.00. running mean: -20.38\n",
      "resetting env. episode reward total was -21.00. running mean: -20.38\n",
      "resetting env. episode reward total was -20.00. running mean: -20.38\n",
      "resetting env. episode reward total was -19.00. running mean: -20.37\n",
      "resetting env. episode reward total was -21.00. running mean: -20.37\n",
      "resetting env. episode reward total was -20.00. running mean: -20.37\n",
      "resetting env. episode reward total was -20.00. running mean: -20.36\n",
      "resetting env. episode reward total was -21.00. running mean: -20.37\n",
      "resetting env. episode reward total was -21.00. running mean: -20.38\n",
      "resetting env. episode reward total was -21.00. running mean: -20.38\n",
      "resetting env. episode reward total was -18.00. running mean: -20.36\n",
      "resetting env. episode reward total was -20.00. running mean: -20.36\n",
      "resetting env. episode reward total was -19.00. running mean: -20.34\n",
      "resetting env. episode reward total was -20.00. running mean: -20.34\n",
      "resetting env. episode reward total was -19.00. running mean: -20.33\n",
      "resetting env. episode reward total was -21.00. running mean: -20.33\n",
      "resetting env. episode reward total was -19.00. running mean: -20.32\n",
      "resetting env. episode reward total was -20.00. running mean: -20.32\n",
      "resetting env. episode reward total was -20.00. running mean: -20.31\n",
      "resetting env. episode reward total was -21.00. running mean: -20.32\n",
      "resetting env. episode reward total was -21.00. running mean: -20.33\n",
      "resetting env. episode reward total was -20.00. running mean: -20.32\n",
      "resetting env. episode reward total was -20.00. running mean: -20.32\n",
      "resetting env. episode reward total was -21.00. running mean: -20.33\n",
      "resetting env. episode reward total was -21.00. running mean: -20.33\n",
      "resetting env. episode reward total was -20.00. running mean: -20.33\n",
      "resetting env. episode reward total was -20.00. running mean: -20.33\n",
      "resetting env. episode reward total was -19.00. running mean: -20.31\n",
      "resetting env. episode reward total was -17.00. running mean: -20.28\n",
      "resetting env. episode reward total was -21.00. running mean: -20.29\n",
      "resetting env. episode reward total was -19.00. running mean: -20.27\n",
      "resetting env. episode reward total was -19.00. running mean: -20.26\n",
      "resetting env. episode reward total was -21.00. running mean: -20.27\n",
      "resetting env. episode reward total was -18.00. running mean: -20.25\n",
      "resetting env. episode reward total was -20.00. running mean: -20.24\n",
      "resetting env. episode reward total was -18.00. running mean: -20.22\n",
      "resetting env. episode reward total was -20.00. running mean: -20.22\n",
      "resetting env. episode reward total was -17.00. running mean: -20.19\n",
      "resetting env. episode reward total was -21.00. running mean: -20.20\n",
      "resetting env. episode reward total was -19.00. running mean: -20.18\n",
      "resetting env. episode reward total was -17.00. running mean: -20.15\n",
      "resetting env. episode reward total was -20.00. running mean: -20.15\n",
      "resetting env. episode reward total was -20.00. running mean: -20.15\n",
      "resetting env. episode reward total was -21.00. running mean: -20.16\n",
      "resetting env. episode reward total was -20.00. running mean: -20.16\n",
      "resetting env. episode reward total was -20.00. running mean: -20.15\n",
      "resetting env. episode reward total was -21.00. running mean: -20.16\n",
      "resetting env. episode reward total was -21.00. running mean: -20.17\n",
      "resetting env. episode reward total was -19.00. running mean: -20.16\n",
      "resetting env. episode reward total was -20.00. running mean: -20.16\n",
      "resetting env. episode reward total was -21.00. running mean: -20.17\n",
      "resetting env. episode reward total was -19.00. running mean: -20.15\n",
      "resetting env. episode reward total was -18.00. running mean: -20.13\n",
      "resetting env. episode reward total was -21.00. running mean: -20.14\n",
      "resetting env. episode reward total was -21.00. running mean: -20.15\n",
      "resetting env. episode reward total was -20.00. running mean: -20.15\n",
      "resetting env. episode reward total was -20.00. running mean: -20.15\n",
      "resetting env. episode reward total was -20.00. running mean: -20.15\n",
      "resetting env. episode reward total was -20.00. running mean: -20.14\n",
      "resetting env. episode reward total was -20.00. running mean: -20.14\n",
      "resetting env. episode reward total was -21.00. running mean: -20.15\n",
      "resetting env. episode reward total was -21.00. running mean: -20.16\n",
      "resetting env. episode reward total was -19.00. running mean: -20.15\n",
      "resetting env. episode reward total was -19.00. running mean: -20.14\n",
      "resetting env. episode reward total was -20.00. running mean: -20.14\n",
      "resetting env. episode reward total was -20.00. running mean: -20.13\n",
      "resetting env. episode reward total was -20.00. running mean: -20.13\n",
      "resetting env. episode reward total was -19.00. running mean: -20.12\n",
      "resetting env. episode reward total was -20.00. running mean: -20.12\n",
      "resetting env. episode reward total was -19.00. running mean: -20.11\n",
      "resetting env. episode reward total was -21.00. running mean: -20.12\n",
      "resetting env. episode reward total was -21.00. running mean: -20.13\n",
      "resetting env. episode reward total was -21.00. running mean: -20.14\n",
      "resetting env. episode reward total was -21.00. running mean: -20.14\n",
      "resetting env. episode reward total was -20.00. running mean: -20.14\n",
      "resetting env. episode reward total was -20.00. running mean: -20.14\n",
      "resetting env. episode reward total was -17.00. running mean: -20.11\n",
      "resetting env. episode reward total was -18.00. running mean: -20.09\n",
      "resetting env. episode reward total was -20.00. running mean: -20.09\n",
      "resetting env. episode reward total was -21.00. running mean: -20.10\n",
      "resetting env. episode reward total was -20.00. running mean: -20.10\n",
      "resetting env. episode reward total was -20.00. running mean: -20.09\n",
      "resetting env. episode reward total was -20.00. running mean: -20.09\n",
      "resetting env. episode reward total was -19.00. running mean: -20.08\n",
      "resetting env. episode reward total was -19.00. running mean: -20.07\n",
      "resetting env. episode reward total was -21.00. running mean: -20.08\n",
      "resetting env. episode reward total was -20.00. running mean: -20.08\n",
      "resetting env. episode reward total was -19.00. running mean: -20.07\n",
      "resetting env. episode reward total was -20.00. running mean: -20.07\n",
      "resetting env. episode reward total was -19.00. running mean: -20.06\n",
      "resetting env. episode reward total was -21.00. running mean: -20.07\n",
      "resetting env. episode reward total was -20.00. running mean: -20.07\n",
      "resetting env. episode reward total was -21.00. running mean: -20.08\n",
      "resetting env. episode reward total was -20.00. running mean: -20.08\n",
      "resetting env. episode reward total was -20.00. running mean: -20.07\n",
      "resetting env. episode reward total was -20.00. running mean: -20.07\n",
      "resetting env. episode reward total was -19.00. running mean: -20.06\n",
      "resetting env. episode reward total was -20.00. running mean: -20.06\n",
      "resetting env. episode reward total was -19.00. running mean: -20.05\n",
      "resetting env. episode reward total was -20.00. running mean: -20.05\n",
      "resetting env. episode reward total was -19.00. running mean: -20.04\n",
      "resetting env. episode reward total was -21.00. running mean: -20.05\n",
      "resetting env. episode reward total was -19.00. running mean: -20.04\n",
      "resetting env. episode reward total was -21.00. running mean: -20.05\n",
      "resetting env. episode reward total was -19.00. running mean: -20.04\n",
      "resetting env. episode reward total was -19.00. running mean: -20.03\n",
      "resetting env. episode reward total was -20.00. running mean: -20.03\n",
      "resetting env. episode reward total was -20.00. running mean: -20.03\n",
      "resetting env. episode reward total was -20.00. running mean: -20.03\n",
      "resetting env. episode reward total was -19.00. running mean: -20.02\n",
      "resetting env. episode reward total was -19.00. running mean: -20.01\n",
      "resetting env. episode reward total was -19.00. running mean: -20.00\n",
      "resetting env. episode reward total was -19.00. running mean: -19.99\n",
      "resetting env. episode reward total was -18.00. running mean: -19.97\n",
      "resetting env. episode reward total was -21.00. running mean: -19.98\n",
      "resetting env. episode reward total was -21.00. running mean: -19.99\n",
      "resetting env. episode reward total was -21.00. running mean: -20.00\n",
      "resetting env. episode reward total was -17.00. running mean: -19.97\n",
      "resetting env. episode reward total was -21.00. running mean: -19.98\n",
      "resetting env. episode reward total was -19.00. running mean: -19.97\n",
      "resetting env. episode reward total was -21.00. running mean: -19.98\n",
      "resetting env. episode reward total was -19.00. running mean: -19.97\n",
      "resetting env. episode reward total was -21.00. running mean: -19.98\n",
      "resetting env. episode reward total was -19.00. running mean: -19.97\n",
      "resetting env. episode reward total was -20.00. running mean: -19.97\n",
      "resetting env. episode reward total was -21.00. running mean: -19.98\n",
      "resetting env. episode reward total was -21.00. running mean: -19.99\n",
      "resetting env. episode reward total was -20.00. running mean: -19.99\n",
      "resetting env. episode reward total was -17.00. running mean: -19.96\n",
      "resetting env. episode reward total was -18.00. running mean: -19.94\n",
      "resetting env. episode reward total was -20.00. running mean: -19.94\n",
      "resetting env. episode reward total was -21.00. running mean: -19.95\n",
      "resetting env. episode reward total was -20.00. running mean: -19.95\n",
      "resetting env. episode reward total was -20.00. running mean: -19.95\n",
      "resetting env. episode reward total was -21.00. running mean: -19.96\n",
      "resetting env. episode reward total was -19.00. running mean: -19.95\n",
      "resetting env. episode reward total was -20.00. running mean: -19.95\n",
      "resetting env. episode reward total was -19.00. running mean: -19.95\n",
      "resetting env. episode reward total was -20.00. running mean: -19.95\n",
      "resetting env. episode reward total was -16.00. running mean: -19.91\n",
      "resetting env. episode reward total was -19.00. running mean: -19.90\n",
      "resetting env. episode reward total was -20.00. running mean: -19.90\n",
      "resetting env. episode reward total was -21.00. running mean: -19.91\n",
      "resetting env. episode reward total was -20.00. running mean: -19.91\n",
      "resetting env. episode reward total was -19.00. running mean: -19.90\n",
      "resetting env. episode reward total was -18.00. running mean: -19.88\n",
      "resetting env. episode reward total was -19.00. running mean: -19.87\n",
      "resetting env. episode reward total was -20.00. running mean: -19.87\n",
      "resetting env. episode reward total was -20.00. running mean: -19.88\n",
      "resetting env. episode reward total was -20.00. running mean: -19.88\n",
      "resetting env. episode reward total was -20.00. running mean: -19.88\n",
      "resetting env. episode reward total was -21.00. running mean: -19.89\n",
      "resetting env. episode reward total was -18.00. running mean: -19.87\n",
      "resetting env. episode reward total was -18.00. running mean: -19.85\n",
      "resetting env. episode reward total was -18.00. running mean: -19.83\n",
      "resetting env. episode reward total was -21.00. running mean: -19.84\n",
      "resetting env. episode reward total was -19.00. running mean: -19.84\n",
      "resetting env. episode reward total was -21.00. running mean: -19.85\n",
      "resetting env. episode reward total was -17.00. running mean: -19.82\n",
      "resetting env. episode reward total was -20.00. running mean: -19.82\n",
      "resetting env. episode reward total was -19.00. running mean: -19.81\n",
      "resetting env. episode reward total was -19.00. running mean: -19.81\n",
      "resetting env. episode reward total was -21.00. running mean: -19.82\n",
      "resetting env. episode reward total was -20.00. running mean: -19.82\n",
      "resetting env. episode reward total was -21.00. running mean: -19.83\n",
      "resetting env. episode reward total was -21.00. running mean: -19.84\n",
      "resetting env. episode reward total was -21.00. running mean: -19.85\n",
      "resetting env. episode reward total was -19.00. running mean: -19.85\n",
      "resetting env. episode reward total was -20.00. running mean: -19.85\n",
      "resetting env. episode reward total was -21.00. running mean: -19.86\n",
      "resetting env. episode reward total was -18.00. running mean: -19.84\n",
      "resetting env. episode reward total was -21.00. running mean: -19.85\n",
      "resetting env. episode reward total was -20.00. running mean: -19.85\n",
      "resetting env. episode reward total was -20.00. running mean: -19.85\n",
      "resetting env. episode reward total was -19.00. running mean: -19.85\n",
      "resetting env. episode reward total was -18.00. running mean: -19.83\n",
      "resetting env. episode reward total was -20.00. running mean: -19.83\n",
      "resetting env. episode reward total was -20.00. running mean: -19.83\n",
      "resetting env. episode reward total was -21.00. running mean: -19.84\n",
      "resetting env. episode reward total was -20.00. running mean: -19.84\n",
      "resetting env. episode reward total was -21.00. running mean: -19.86\n",
      "resetting env. episode reward total was -18.00. running mean: -19.84\n",
      "resetting env. episode reward total was -20.00. running mean: -19.84\n",
      "resetting env. episode reward total was -21.00. running mean: -19.85\n",
      "resetting env. episode reward total was -18.00. running mean: -19.83\n",
      "resetting env. episode reward total was -21.00. running mean: -19.84\n",
      "resetting env. episode reward total was -19.00. running mean: -19.84\n",
      "resetting env. episode reward total was -21.00. running mean: -19.85\n",
      "resetting env. episode reward total was -20.00. running mean: -19.85\n",
      "resetting env. episode reward total was -20.00. running mean: -19.85\n",
      "resetting env. episode reward total was -20.00. running mean: -19.85\n",
      "resetting env. episode reward total was -20.00. running mean: -19.85\n",
      "resetting env. episode reward total was -20.00. running mean: -19.85\n",
      "resetting env. episode reward total was -20.00. running mean: -19.86\n",
      "resetting env. episode reward total was -20.00. running mean: -19.86\n",
      "resetting env. episode reward total was -20.00. running mean: -19.86\n",
      "resetting env. episode reward total was -21.00. running mean: -19.87\n",
      "resetting env. episode reward total was -20.00. running mean: -19.87\n",
      "resetting env. episode reward total was -20.00. running mean: -19.87\n",
      "resetting env. episode reward total was -20.00. running mean: -19.87\n",
      "resetting env. episode reward total was -20.00. running mean: -19.88\n",
      "resetting env. episode reward total was -21.00. running mean: -19.89\n",
      "resetting env. episode reward total was -19.00. running mean: -19.88\n",
      "resetting env. episode reward total was -20.00. running mean: -19.88\n",
      "resetting env. episode reward total was -20.00. running mean: -19.88\n",
      "resetting env. episode reward total was -21.00. running mean: -19.89\n",
      "resetting env. episode reward total was -21.00. running mean: -19.90\n",
      "resetting env. episode reward total was -19.00. running mean: -19.89\n",
      "resetting env. episode reward total was -20.00. running mean: -19.89\n",
      "resetting env. episode reward total was -19.00. running mean: -19.89\n",
      "resetting env. episode reward total was -20.00. running mean: -19.89\n",
      "resetting env. episode reward total was -18.00. running mean: -19.87\n",
      "resetting env. episode reward total was -21.00. running mean: -19.88\n",
      "resetting env. episode reward total was -20.00. running mean: -19.88\n",
      "resetting env. episode reward total was -20.00. running mean: -19.88\n",
      "resetting env. episode reward total was -20.00. running mean: -19.88\n",
      "resetting env. episode reward total was -19.00. running mean: -19.87\n",
      "resetting env. episode reward total was -21.00. running mean: -19.88\n",
      "resetting env. episode reward total was -18.00. running mean: -19.87\n",
      "resetting env. episode reward total was -21.00. running mean: -19.88\n",
      "resetting env. episode reward total was -17.00. running mean: -19.85\n",
      "resetting env. episode reward total was -19.00. running mean: -19.84\n",
      "resetting env. episode reward total was -19.00. running mean: -19.83\n",
      "resetting env. episode reward total was -19.00. running mean: -19.82\n",
      "resetting env. episode reward total was -20.00. running mean: -19.83\n",
      "resetting env. episode reward total was -20.00. running mean: -19.83\n",
      "resetting env. episode reward total was -19.00. running mean: -19.82\n",
      "resetting env. episode reward total was -18.00. running mean: -19.80\n",
      "resetting env. episode reward total was -19.00. running mean: -19.79\n",
      "resetting env. episode reward total was -21.00. running mean: -19.80\n",
      "resetting env. episode reward total was -21.00. running mean: -19.82\n",
      "resetting env. episode reward total was -20.00. running mean: -19.82\n",
      "resetting env. episode reward total was -21.00. running mean: -19.83\n",
      "resetting env. episode reward total was -21.00. running mean: -19.84\n",
      "resetting env. episode reward total was -21.00. running mean: -19.85\n",
      "resetting env. episode reward total was -20.00. running mean: -19.85\n",
      "resetting env. episode reward total was -20.00. running mean: -19.86\n",
      "resetting env. episode reward total was -18.00. running mean: -19.84\n",
      "resetting env. episode reward total was -20.00. running mean: -19.84\n",
      "resetting env. episode reward total was -21.00. running mean: -19.85\n",
      "resetting env. episode reward total was -21.00. running mean: -19.86\n",
      "resetting env. episode reward total was -21.00. running mean: -19.87\n",
      "resetting env. episode reward total was -18.00. running mean: -19.86\n",
      "resetting env. episode reward total was -20.00. running mean: -19.86\n",
      "resetting env. episode reward total was -21.00. running mean: -19.87\n",
      "resetting env. episode reward total was -19.00. running mean: -19.86\n",
      "resetting env. episode reward total was -20.00. running mean: -19.86\n",
      "resetting env. episode reward total was -21.00. running mean: -19.87\n",
      "resetting env. episode reward total was -17.00. running mean: -19.84\n",
      "resetting env. episode reward total was -20.00. running mean: -19.85\n",
      "resetting env. episode reward total was -16.00. running mean: -19.81\n",
      "resetting env. episode reward total was -19.00. running mean: -19.80\n",
      "resetting env. episode reward total was -21.00. running mean: -19.81\n",
      "resetting env. episode reward total was -17.00. running mean: -19.78\n",
      "resetting env. episode reward total was -20.00. running mean: -19.78\n",
      "resetting env. episode reward total was -20.00. running mean: -19.79\n",
      "resetting env. episode reward total was -19.00. running mean: -19.78\n",
      "resetting env. episode reward total was -20.00. running mean: -19.78\n",
      "resetting env. episode reward total was -20.00. running mean: -19.78\n",
      "resetting env. episode reward total was -21.00. running mean: -19.80\n",
      "resetting env. episode reward total was -20.00. running mean: -19.80\n",
      "resetting env. episode reward total was -20.00. running mean: -19.80\n",
      "resetting env. episode reward total was -20.00. running mean: -19.80\n",
      "resetting env. episode reward total was -21.00. running mean: -19.81\n",
      "resetting env. episode reward total was -20.00. running mean: -19.82\n",
      "resetting env. episode reward total was -21.00. running mean: -19.83\n",
      "resetting env. episode reward total was -19.00. running mean: -19.82\n",
      "resetting env. episode reward total was -20.00. running mean: -19.82\n",
      "resetting env. episode reward total was -20.00. running mean: -19.82\n",
      "resetting env. episode reward total was -18.00. running mean: -19.80\n",
      "resetting env. episode reward total was -19.00. running mean: -19.80\n",
      "resetting env. episode reward total was -20.00. running mean: -19.80\n",
      "resetting env. episode reward total was -19.00. running mean: -19.79\n",
      "resetting env. episode reward total was -19.00. running mean: -19.78\n",
      "resetting env. episode reward total was -20.00. running mean: -19.78\n",
      "resetting env. episode reward total was -21.00. running mean: -19.80\n",
      "resetting env. episode reward total was -17.00. running mean: -19.77\n",
      "resetting env. episode reward total was -17.00. running mean: -19.74\n",
      "resetting env. episode reward total was -21.00. running mean: -19.75\n",
      "resetting env. episode reward total was -21.00. running mean: -19.77\n",
      "resetting env. episode reward total was -18.00. running mean: -19.75\n",
      "resetting env. episode reward total was -16.00. running mean: -19.71\n",
      "resetting env. episode reward total was -19.00. running mean: -19.70\n",
      "resetting env. episode reward total was -19.00. running mean: -19.70\n",
      "resetting env. episode reward total was -19.00. running mean: -19.69\n",
      "resetting env. episode reward total was -21.00. running mean: -19.70\n",
      "resetting env. episode reward total was -20.00. running mean: -19.71\n",
      "resetting env. episode reward total was -21.00. running mean: -19.72\n",
      "resetting env. episode reward total was -17.00. running mean: -19.69\n",
      "resetting env. episode reward total was -20.00. running mean: -19.69\n",
      "resetting env. episode reward total was -19.00. running mean: -19.69\n",
      "resetting env. episode reward total was -20.00. running mean: -19.69\n",
      "resetting env. episode reward total was -20.00. running mean: -19.69\n",
      "resetting env. episode reward total was -20.00. running mean: -19.70\n",
      "resetting env. episode reward total was -18.00. running mean: -19.68\n",
      "resetting env. episode reward total was -20.00. running mean: -19.68\n",
      "resetting env. episode reward total was -18.00. running mean: -19.67\n",
      "resetting env. episode reward total was -21.00. running mean: -19.68\n",
      "resetting env. episode reward total was -18.00. running mean: -19.66\n",
      "resetting env. episode reward total was -20.00. running mean: -19.67\n",
      "resetting env. episode reward total was -19.00. running mean: -19.66\n",
      "resetting env. episode reward total was -19.00. running mean: -19.65\n",
      "resetting env. episode reward total was -18.00. running mean: -19.64\n",
      "resetting env. episode reward total was -19.00. running mean: -19.63\n",
      "resetting env. episode reward total was -21.00. running mean: -19.64\n",
      "resetting env. episode reward total was -21.00. running mean: -19.66\n",
      "resetting env. episode reward total was -18.00. running mean: -19.64\n",
      "resetting env. episode reward total was -20.00. running mean: -19.64\n",
      "resetting env. episode reward total was -21.00. running mean: -19.66\n",
      "resetting env. episode reward total was -19.00. running mean: -19.65\n",
      "resetting env. episode reward total was -19.00. running mean: -19.65\n",
      "resetting env. episode reward total was -20.00. running mean: -19.65\n",
      "resetting env. episode reward total was -21.00. running mean: -19.66\n",
      "resetting env. episode reward total was -21.00. running mean: -19.68\n",
      "resetting env. episode reward total was -18.00. running mean: -19.66\n",
      "resetting env. episode reward total was -21.00. running mean: -19.67\n",
      "resetting env. episode reward total was -20.00. running mean: -19.68\n",
      "resetting env. episode reward total was -20.00. running mean: -19.68\n",
      "resetting env. episode reward total was -18.00. running mean: -19.66\n",
      "resetting env. episode reward total was -20.00. running mean: -19.67\n",
      "resetting env. episode reward total was -20.00. running mean: -19.67\n",
      "resetting env. episode reward total was -20.00. running mean: -19.67\n",
      "resetting env. episode reward total was -20.00. running mean: -19.68\n",
      "resetting env. episode reward total was -18.00. running mean: -19.66\n",
      "resetting env. episode reward total was -21.00. running mean: -19.67\n",
      "resetting env. episode reward total was -21.00. running mean: -19.69\n",
      "resetting env. episode reward total was -18.00. running mean: -19.67\n",
      "resetting env. episode reward total was -21.00. running mean: -19.68\n",
      "resetting env. episode reward total was -20.00. running mean: -19.68\n",
      "resetting env. episode reward total was -18.00. running mean: -19.67\n",
      "resetting env. episode reward total was -21.00. running mean: -19.68\n",
      "resetting env. episode reward total was -20.00. running mean: -19.68\n",
      "resetting env. episode reward total was -20.00. running mean: -19.69\n",
      "resetting env. episode reward total was -19.00. running mean: -19.68\n",
      "resetting env. episode reward total was -19.00. running mean: -19.67\n",
      "resetting env. episode reward total was -17.00. running mean: -19.65\n",
      "resetting env. episode reward total was -18.00. running mean: -19.63\n",
      "resetting env. episode reward total was -19.00. running mean: -19.62\n",
      "resetting env. episode reward total was -19.00. running mean: -19.62\n",
      "resetting env. episode reward total was -20.00. running mean: -19.62\n",
      "resetting env. episode reward total was -20.00. running mean: -19.63\n",
      "resetting env. episode reward total was -20.00. running mean: -19.63\n",
      "resetting env. episode reward total was -18.00. running mean: -19.61\n",
      "resetting env. episode reward total was -20.00. running mean: -19.62\n",
      "resetting env. episode reward total was -19.00. running mean: -19.61\n",
      "resetting env. episode reward total was -18.00. running mean: -19.59\n",
      "resetting env. episode reward total was -17.00. running mean: -19.57\n",
      "resetting env. episode reward total was -18.00. running mean: -19.55\n",
      "resetting env. episode reward total was -21.00. running mean: -19.57\n",
      "resetting env. episode reward total was -20.00. running mean: -19.57\n",
      "resetting env. episode reward total was -18.00. running mean: -19.56\n",
      "resetting env. episode reward total was -19.00. running mean: -19.55\n",
      "resetting env. episode reward total was -20.00. running mean: -19.56\n",
      "resetting env. episode reward total was -19.00. running mean: -19.55\n",
      "resetting env. episode reward total was -19.00. running mean: -19.54\n",
      "resetting env. episode reward total was -20.00. running mean: -19.55\n",
      "resetting env. episode reward total was -19.00. running mean: -19.54\n",
      "resetting env. episode reward total was -21.00. running mean: -19.56\n",
      "resetting env. episode reward total was -21.00. running mean: -19.57\n",
      "resetting env. episode reward total was -20.00. running mean: -19.58\n",
      "resetting env. episode reward total was -18.00. running mean: -19.56\n",
      "resetting env. episode reward total was -17.00. running mean: -19.54\n",
      "resetting env. episode reward total was -19.00. running mean: -19.53\n",
      "resetting env. episode reward total was -19.00. running mean: -19.52\n",
      "resetting env. episode reward total was -21.00. running mean: -19.54\n",
      "resetting env. episode reward total was -17.00. running mean: -19.51\n",
      "resetting env. episode reward total was -21.00. running mean: -19.53\n",
      "resetting env. episode reward total was -20.00. running mean: -19.53\n",
      "resetting env. episode reward total was -16.00. running mean: -19.50\n",
      "resetting env. episode reward total was -20.00. running mean: -19.50\n",
      "resetting env. episode reward total was -20.00. running mean: -19.51\n",
      "resetting env. episode reward total was -19.00. running mean: -19.50\n",
      "resetting env. episode reward total was -15.00. running mean: -19.46\n",
      "resetting env. episode reward total was -18.00. running mean: -19.44\n",
      "resetting env. episode reward total was -17.00. running mean: -19.42\n",
      "resetting env. episode reward total was -21.00. running mean: -19.43\n",
      "resetting env. episode reward total was -21.00. running mean: -19.45\n",
      "resetting env. episode reward total was -16.00. running mean: -19.42\n",
      "resetting env. episode reward total was -20.00. running mean: -19.42\n",
      "resetting env. episode reward total was -19.00. running mean: -19.42\n",
      "resetting env. episode reward total was -20.00. running mean: -19.42\n",
      "resetting env. episode reward total was -19.00. running mean: -19.42\n",
      "resetting env. episode reward total was -21.00. running mean: -19.43\n",
      "resetting env. episode reward total was -20.00. running mean: -19.44\n",
      "resetting env. episode reward total was -17.00. running mean: -19.42\n",
      "resetting env. episode reward total was -21.00. running mean: -19.43\n",
      "resetting env. episode reward total was -19.00. running mean: -19.43\n",
      "resetting env. episode reward total was -20.00. running mean: -19.43\n",
      "resetting env. episode reward total was -20.00. running mean: -19.44\n",
      "resetting env. episode reward total was -19.00. running mean: -19.43\n",
      "resetting env. episode reward total was -17.00. running mean: -19.41\n",
      "resetting env. episode reward total was -20.00. running mean: -19.42\n",
      "resetting env. episode reward total was -18.00. running mean: -19.40\n",
      "resetting env. episode reward total was -21.00. running mean: -19.42\n",
      "resetting env. episode reward total was -20.00. running mean: -19.42\n",
      "resetting env. episode reward total was -17.00. running mean: -19.40\n",
      "resetting env. episode reward total was -19.00. running mean: -19.40\n",
      "resetting env. episode reward total was -17.00. running mean: -19.37\n",
      "resetting env. episode reward total was -16.00. running mean: -19.34\n",
      "resetting env. episode reward total was -20.00. running mean: -19.34\n",
      "resetting env. episode reward total was -19.00. running mean: -19.34\n",
      "resetting env. episode reward total was -20.00. running mean: -19.35\n",
      "resetting env. episode reward total was -19.00. running mean: -19.34\n",
      "resetting env. episode reward total was -17.00. running mean: -19.32\n",
      "resetting env. episode reward total was -15.00. running mean: -19.28\n",
      "resetting env. episode reward total was -19.00. running mean: -19.27\n",
      "resetting env. episode reward total was -21.00. running mean: -19.29\n",
      "resetting env. episode reward total was -21.00. running mean: -19.31\n",
      "resetting env. episode reward total was -17.00. running mean: -19.29\n",
      "resetting env. episode reward total was -18.00. running mean: -19.27\n",
      "resetting env. episode reward total was -18.00. running mean: -19.26\n",
      "resetting env. episode reward total was -19.00. running mean: -19.26\n",
      "resetting env. episode reward total was -18.00. running mean: -19.25\n",
      "resetting env. episode reward total was -18.00. running mean: -19.23\n",
      "resetting env. episode reward total was -18.00. running mean: -19.22\n",
      "resetting env. episode reward total was -17.00. running mean: -19.20\n",
      "resetting env. episode reward total was -16.00. running mean: -19.17\n",
      "resetting env. episode reward total was -17.00. running mean: -19.14\n",
      "resetting env. episode reward total was -18.00. running mean: -19.13\n",
      "resetting env. episode reward total was -17.00. running mean: -19.11\n",
      "resetting env. episode reward total was -21.00. running mean: -19.13\n",
      "resetting env. episode reward total was -20.00. running mean: -19.14\n",
      "resetting env. episode reward total was -16.00. running mean: -19.11\n",
      "resetting env. episode reward total was -21.00. running mean: -19.13\n",
      "resetting env. episode reward total was -20.00. running mean: -19.14\n",
      "resetting env. episode reward total was -20.00. running mean: -19.14\n",
      "resetting env. episode reward total was -20.00. running mean: -19.15\n",
      "resetting env. episode reward total was -17.00. running mean: -19.13\n",
      "resetting env. episode reward total was -21.00. running mean: -19.15\n",
      "resetting env. episode reward total was -21.00. running mean: -19.17\n",
      "resetting env. episode reward total was -19.00. running mean: -19.17\n",
      "resetting env. episode reward total was -18.00. running mean: -19.16\n",
      "resetting env. episode reward total was -20.00. running mean: -19.16\n",
      "resetting env. episode reward total was -19.00. running mean: -19.16\n",
      "resetting env. episode reward total was -17.00. running mean: -19.14\n",
      "resetting env. episode reward total was -20.00. running mean: -19.15\n",
      "resetting env. episode reward total was -20.00. running mean: -19.16\n",
      "resetting env. episode reward total was -21.00. running mean: -19.18\n",
      "resetting env. episode reward total was -20.00. running mean: -19.18\n",
      "resetting env. episode reward total was -19.00. running mean: -19.18\n",
      "resetting env. episode reward total was -19.00. running mean: -19.18\n",
      "resetting env. episode reward total was -19.00. running mean: -19.18\n",
      "resetting env. episode reward total was -15.00. running mean: -19.14\n",
      "resetting env. episode reward total was -18.00. running mean: -19.13\n",
      "resetting env. episode reward total was -19.00. running mean: -19.12\n",
      "resetting env. episode reward total was -16.00. running mean: -19.09\n",
      "resetting env. episode reward total was -17.00. running mean: -19.07\n",
      "resetting env. episode reward total was -15.00. running mean: -19.03\n",
      "resetting env. episode reward total was -19.00. running mean: -19.03\n",
      "resetting env. episode reward total was -19.00. running mean: -19.03\n",
      "resetting env. episode reward total was -17.00. running mean: -19.01\n",
      "resetting env. episode reward total was -19.00. running mean: -19.01\n",
      "resetting env. episode reward total was -20.00. running mean: -19.02\n",
      "resetting env. episode reward total was -19.00. running mean: -19.02\n",
      "resetting env. episode reward total was -20.00. running mean: -19.03\n",
      "resetting env. episode reward total was -16.00. running mean: -19.00\n",
      "resetting env. episode reward total was -19.00. running mean: -19.00\n",
      "resetting env. episode reward total was -19.00. running mean: -19.00\n",
      "resetting env. episode reward total was -20.00. running mean: -19.01\n",
      "resetting env. episode reward total was -15.00. running mean: -18.97\n",
      "resetting env. episode reward total was -19.00. running mean: -18.97\n",
      "resetting env. episode reward total was -19.00. running mean: -18.97\n",
      "resetting env. episode reward total was -20.00. running mean: -18.98\n",
      "resetting env. episode reward total was -19.00. running mean: -18.98\n",
      "resetting env. episode reward total was -15.00. running mean: -18.94\n",
      "resetting env. episode reward total was -17.00. running mean: -18.92\n",
      "resetting env. episode reward total was -16.00. running mean: -18.89\n",
      "resetting env. episode reward total was -21.00. running mean: -18.91\n",
      "resetting env. episode reward total was -20.00. running mean: -18.92\n",
      "resetting env. episode reward total was -18.00. running mean: -18.91\n",
      "resetting env. episode reward total was -21.00. running mean: -18.94\n",
      "resetting env. episode reward total was -20.00. running mean: -18.95\n",
      "resetting env. episode reward total was -18.00. running mean: -18.94\n",
      "resetting env. episode reward total was -20.00. running mean: -18.95\n",
      "resetting env. episode reward total was -20.00. running mean: -18.96\n",
      "resetting env. episode reward total was -19.00. running mean: -18.96\n",
      "resetting env. episode reward total was -20.00. running mean: -18.97\n",
      "resetting env. episode reward total was -19.00. running mean: -18.97\n",
      "resetting env. episode reward total was -20.00. running mean: -18.98\n",
      "resetting env. episode reward total was -16.00. running mean: -18.95\n",
      "resetting env. episode reward total was -19.00. running mean: -18.95\n",
      "resetting env. episode reward total was -15.00. running mean: -18.91\n",
      "resetting env. episode reward total was -20.00. running mean: -18.92\n",
      "resetting env. episode reward total was -21.00. running mean: -18.94\n",
      "resetting env. episode reward total was -20.00. running mean: -18.95\n",
      "resetting env. episode reward total was -20.00. running mean: -18.96\n",
      "resetting env. episode reward total was -15.00. running mean: -18.92\n",
      "resetting env. episode reward total was -19.00. running mean: -18.92\n",
      "resetting env. episode reward total was -19.00. running mean: -18.93\n",
      "resetting env. episode reward total was -19.00. running mean: -18.93\n",
      "resetting env. episode reward total was -17.00. running mean: -18.91\n",
      "resetting env. episode reward total was -20.00. running mean: -18.92\n",
      "resetting env. episode reward total was -20.00. running mean: -18.93\n",
      "resetting env. episode reward total was -16.00. running mean: -18.90\n",
      "resetting env. episode reward total was -17.00. running mean: -18.88\n",
      "resetting env. episode reward total was -19.00. running mean: -18.88\n",
      "resetting env. episode reward total was -14.00. running mean: -18.83\n",
      "resetting env. episode reward total was -20.00. running mean: -18.84\n",
      "resetting env. episode reward total was -19.00. running mean: -18.85\n",
      "resetting env. episode reward total was -21.00. running mean: -18.87\n",
      "resetting env. episode reward total was -15.00. running mean: -18.83\n",
      "resetting env. episode reward total was -17.00. running mean: -18.81\n",
      "resetting env. episode reward total was -19.00. running mean: -18.81\n",
      "resetting env. episode reward total was -19.00. running mean: -18.81\n",
      "resetting env. episode reward total was -21.00. running mean: -18.84\n",
      "resetting env. episode reward total was -21.00. running mean: -18.86\n",
      "resetting env. episode reward total was -17.00. running mean: -18.84\n",
      "resetting env. episode reward total was -21.00. running mean: -18.86\n",
      "resetting env. episode reward total was -19.00. running mean: -18.86\n",
      "resetting env. episode reward total was -20.00. running mean: -18.87\n",
      "resetting env. episode reward total was -20.00. running mean: -18.88\n",
      "resetting env. episode reward total was -18.00. running mean: -18.88\n",
      "resetting env. episode reward total was -13.00. running mean: -18.82\n",
      "resetting env. episode reward total was -18.00. running mean: -18.81\n",
      "resetting env. episode reward total was -21.00. running mean: -18.83\n",
      "resetting env. episode reward total was -21.00. running mean: -18.85\n",
      "resetting env. episode reward total was -18.00. running mean: -18.84\n",
      "resetting env. episode reward total was -19.00. running mean: -18.85\n",
      "resetting env. episode reward total was -21.00. running mean: -18.87\n",
      "resetting env. episode reward total was -20.00. running mean: -18.88\n",
      "resetting env. episode reward total was -17.00. running mean: -18.86\n",
      "resetting env. episode reward total was -20.00. running mean: -18.87\n",
      "resetting env. episode reward total was -21.00. running mean: -18.89\n",
      "resetting env. episode reward total was -19.00. running mean: -18.89\n",
      "resetting env. episode reward total was -21.00. running mean: -18.91\n",
      "resetting env. episode reward total was -18.00. running mean: -18.91\n",
      "resetting env. episode reward total was -20.00. running mean: -18.92\n",
      "resetting env. episode reward total was -19.00. running mean: -18.92\n",
      "resetting env. episode reward total was -19.00. running mean: -18.92\n",
      "resetting env. episode reward total was -18.00. running mean: -18.91\n",
      "resetting env. episode reward total was -18.00. running mean: -18.90\n",
      "resetting env. episode reward total was -20.00. running mean: -18.91\n",
      "resetting env. episode reward total was -19.00. running mean: -18.91\n",
      "resetting env. episode reward total was -18.00. running mean: -18.90\n",
      "resetting env. episode reward total was -19.00. running mean: -18.90\n",
      "resetting env. episode reward total was -18.00. running mean: -18.89\n",
      "resetting env. episode reward total was -21.00. running mean: -18.92\n",
      "resetting env. episode reward total was -18.00. running mean: -18.91\n",
      "resetting env. episode reward total was -17.00. running mean: -18.89\n",
      "resetting env. episode reward total was -16.00. running mean: -18.86\n",
      "resetting env. episode reward total was -18.00. running mean: -18.85\n",
      "resetting env. episode reward total was -18.00. running mean: -18.84\n",
      "resetting env. episode reward total was -19.00. running mean: -18.84\n",
      "resetting env. episode reward total was -17.00. running mean: -18.82\n",
      "resetting env. episode reward total was -19.00. running mean: -18.83\n",
      "resetting env. episode reward total was -20.00. running mean: -18.84\n",
      "resetting env. episode reward total was -20.00. running mean: -18.85\n",
      "resetting env. episode reward total was -19.00. running mean: -18.85\n",
      "resetting env. episode reward total was -21.00. running mean: -18.87\n",
      "resetting env. episode reward total was -19.00. running mean: -18.87\n",
      "resetting env. episode reward total was -18.00. running mean: -18.87\n",
      "resetting env. episode reward total was -19.00. running mean: -18.87\n",
      "resetting env. episode reward total was -18.00. running mean: -18.86\n",
      "resetting env. episode reward total was -17.00. running mean: -18.84\n",
      "resetting env. episode reward total was -20.00. running mean: -18.85\n",
      "resetting env. episode reward total was -21.00. running mean: -18.87\n",
      "resetting env. episode reward total was -13.00. running mean: -18.81\n",
      "resetting env. episode reward total was -18.00. running mean: -18.81\n",
      "resetting env. episode reward total was -17.00. running mean: -18.79\n",
      "resetting env. episode reward total was -21.00. running mean: -18.81\n",
      "resetting env. episode reward total was -21.00. running mean: -18.83\n",
      "resetting env. episode reward total was -18.00. running mean: -18.82\n",
      "resetting env. episode reward total was -18.00. running mean: -18.81\n",
      "resetting env. episode reward total was -20.00. running mean: -18.83\n",
      "resetting env. episode reward total was -16.00. running mean: -18.80\n",
      "resetting env. episode reward total was -18.00. running mean: -18.79\n",
      "resetting env. episode reward total was -19.00. running mean: -18.79\n",
      "resetting env. episode reward total was -17.00. running mean: -18.77\n",
      "resetting env. episode reward total was -18.00. running mean: -18.77\n",
      "resetting env. episode reward total was -21.00. running mean: -18.79\n",
      "resetting env. episode reward total was -19.00. running mean: -18.79\n",
      "resetting env. episode reward total was -18.00. running mean: -18.78\n",
      "resetting env. episode reward total was -17.00. running mean: -18.77\n",
      "resetting env. episode reward total was -18.00. running mean: -18.76\n",
      "resetting env. episode reward total was -21.00. running mean: -18.78\n",
      "resetting env. episode reward total was -21.00. running mean: -18.80\n",
      "resetting env. episode reward total was -16.00. running mean: -18.77\n",
      "resetting env. episode reward total was -19.00. running mean: -18.78\n",
      "resetting env. episode reward total was -18.00. running mean: -18.77\n",
      "resetting env. episode reward total was -15.00. running mean: -18.73\n",
      "resetting env. episode reward total was -18.00. running mean: -18.72\n",
      "resetting env. episode reward total was -17.00. running mean: -18.71\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform() \u001b[38;5;241m<\u001b[39m aprob \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Take a step\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# \"fake label\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Record intermediates based on reward value\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ale_py/env/gym.py:256\u001b[0m, in \u001b[0;36mAtariEnv.step\u001b[0;34m(self, action_ind)\u001b[0m\n\u001b[1;32m    254\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frameskip):\n\u001b[0;32m--> 256\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43male\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m is_terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_over(with_truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    258\u001b[0m is_truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_truncated()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "  if render: env.render()\n",
    "\n",
    "  # Preprocess and difference observation\n",
    "  current_x = prepro(observation)\n",
    "  x = current_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "  prev_x = current_x\n",
    "\n",
    "  # Feed forward policy network\n",
    "  aprob, h = policy_forward(x)\n",
    "  # Sample action from returned probability\n",
    "  action = 2 if np.random.uniform() < aprob else 3\n",
    "\n",
    "  # Take a step\n",
    "  observation, reward, terminated, truncated, info = env.step(action)\n",
    "  y = 1 if action == 2 else 0 # \"fake label\"\n",
    "  \n",
    "  # Record intermediates based on reward value\n",
    "  if running_reward==None or reward < running_reward:\n",
    "    reward_sum_neg += reward\n",
    "    rs.append(reward)\n",
    "    xs.append(x) # observation\n",
    "    hs.append(h) # hidden state\n",
    "    pgrads.append(y - aprob) # policy gradient\n",
    "  else:\n",
    "    reward_sum_pos += reward\n",
    "    rsp.append(reward)\n",
    "    xsp.append(x)\n",
    "    hsp.append(h)\n",
    "    pgradsp.append(y - aprob)\n",
    "\n",
    "  # If episode terminated\n",
    "  if terminated or truncated:\n",
    "    episode_number += 1\n",
    "\n",
    "    # If xs is not empty\n",
    "    if xs:\n",
    "      gradient_buffer = gradient_update(rs, xs, hs, pgrads, gradient_buffer)\n",
    "      # Reset to empty\n",
    "      xs,hs,pgrads,rs = [],[],[],[]\n",
    "    \n",
    "    if xsp:\n",
    "      gradient_buffer = gradient_update(rsp, xsp, hsp, pgradsp, gradient_buffer)\n",
    "      xsp,hsp,pgradsp,rsp = [],[],[],[]\n",
    "\n",
    "    # At end of batch: update model\n",
    "    if episode_number % batch_size == 0:\n",
    "      # For each layer of weights\n",
    "      for k,v in model.items():\n",
    "        # Get summed gradient\n",
    "        g = gradient_buffer[k]\n",
    "\n",
    "        # # ADAM optimizer\n",
    "        # m[k] = b1 * m[k] + (1 - b1) * g\n",
    "        # c[k] = b2 * c[k] + (1 - b2) * g**2\n",
    "\n",
    "        # m_hat = m[k] / (1 - b1**episode_number)\n",
    "        # c_hat = c[k] / (1 - b2**episode_number)\n",
    "\n",
    "        # # Update model\n",
    "        # model[k] += learning_rate * m_hat / (np.sqrt(c_hat) + epsilon)\n",
    "\n",
    "        # # Reset gradient buffer\n",
    "        # gradient_buffer[k] = np.zeros_like(v)\n",
    "\n",
    "        # perform rmsprop parameter update every batch_size episodes\n",
    "        g = gradient_buffer[k] # gradient\n",
    "        rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "        model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "        gradient_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
    "\n",
    "    # Book-keeping\n",
    "    reward_sum = reward_sum_pos + reward_sum_neg\n",
    "    running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "    print('resetting env. episode reward total was %.2f. running mean: %.2f' % (reward_sum, running_reward))\n",
    "    # Log reward\n",
    "    log_reward(episode_number, running_reward)\n",
    "    # Save model at checkpoints\n",
    "    if episode_number % 500 == 0:\n",
    "      pickle.dump(model, open(f'checkpoints/save_exp5{episode_number}.p', 'wb'))\n",
    "\n",
    "    # Reset episode after temination\n",
    "    reward_sum_neg = 0\n",
    "    reward_sum_pos = 0\n",
    "    prev_x = None\n",
    "    observation, info = env.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
